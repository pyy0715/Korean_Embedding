{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T10:29:43.719898Z",
     "start_time": "2020-03-09T10:29:37.884687Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import time\n",
    "import argparse, os, sys\n",
    "from collections import defaultdict\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T10:29:44.441531Z",
     "start_time": "2020-03-09T10:29:44.396246Z"
    }
   },
   "outputs": [],
   "source": [
    "class CBoWModel(object):\n",
    "\n",
    "    def __init__(self, train_fname, embedding_fname, model_fname, embedding_corpus_fname,\n",
    "                 embedding_method=\"fasttext\", is_weighted=True, average=False, dim=100, tokenizer_name=\"mecab\"):\n",
    "        # configurations\n",
    "        make_save_path(model_fname)\n",
    "        self.dim = dim\n",
    "        self.average = average\n",
    "        if is_weighted:\n",
    "            model_full_fname = model_fname + \"-weighted\"\n",
    "        else:\n",
    "            model_full_fname = model_fname + \"-original\"\n",
    "        self.tokenizer = get_tokenizer(tokenizer_name)\n",
    "        if is_weighted:\n",
    "            # ready for weighted embeddings\n",
    "            self.embeddings = self.load_or_construct_weighted_embedding(embedding_fname, embedding_method, embedding_corpus_fname)\n",
    "            print(\"loading weighted embeddings, complete!\")\n",
    "        else:\n",
    "            # ready for original embeddings\n",
    "            words, vectors = self.load_word_embeddings(embedding_fname, embedding_method)\n",
    "            self.embeddings = defaultdict(list)\n",
    "            for word, vector in zip(words, vectors):\n",
    "                self.embeddings[word] = vector\n",
    "            print(\"loading original embeddings, complete!\")\n",
    "        if not os.path.exists(model_full_fname):\n",
    "            print(\"train Continuous Bag of Words model\")\n",
    "            self.model = self.train_model(train_fname, model_full_fname)\n",
    "        else:\n",
    "            print(\"load Continuous Bag of Words model\")\n",
    "            self.model = self.load_model(model_full_fname)\n",
    "\n",
    "    def evaluate(self, test_data_fname, batch_size=3000, verbose=False):\n",
    "        print(\"evaluation start!\")\n",
    "        test_data = self.load_or_tokenize_corpus(test_data_fname)\n",
    "        data_size = len(test_data)\n",
    "        num_batches = int((data_size - 1) / batch_size) + 1\n",
    "        eval_score = 0\n",
    "        for batch_num in range(num_batches):\n",
    "            batch_sentences = []\n",
    "            batch_tokenized_sentences = []\n",
    "            batch_labels = []\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            features = test_data[start_index:end_index]\n",
    "            for feature in features:\n",
    "                sentence, tokens, label = feature\n",
    "                batch_sentences.append(sentence)\n",
    "                batch_tokenized_sentences.append(tokens)\n",
    "                batch_labels.append(label)\n",
    "            preds, curr_eval_score = self.predict_by_batch(batch_tokenized_sentences, batch_labels)\n",
    "            eval_score += curr_eval_score\n",
    "        if verbose:\n",
    "            for sentence, pred, label in zip(batch_sentences, preds, batch_labels):\n",
    "                print(sentence, \", pred:\", pred, \", label:\", label)\n",
    "        print(\"# of correct:\", str(eval_score), \", total:\", str(len(test_data)), \", score:\", str(eval_score / len(test_data)))\n",
    "\n",
    "    def predict(self, sentence):\n",
    "        tokens = self.tokenizer.morphs(sentence)\n",
    "        sentence_vector = self.get_sentence_vector(tokens)\n",
    "        scores = np.dot(self.model[\"vectors\"], sentence_vector)\n",
    "        pred = self.model[\"labels\"][np.argmax(scores)]\n",
    "        return pred\n",
    "\n",
    "    def predict_by_batch(self, tokenized_sentences, labels):\n",
    "        sentence_vectors, eval_score = [], 0\n",
    "        for tokens in tokenized_sentences:\n",
    "            sentence_vectors.append(self.get_sentence_vector(tokens))\n",
    "        scores = np.dot(self.model[\"vectors\"], np.array(sentence_vectors).T)\n",
    "        preds = np.argmax(scores, axis=0)\n",
    "        for pred, label in zip(preds, labels):\n",
    "            if self.model[\"labels\"][pred] == label:\n",
    "                eval_score += 1\n",
    "        return preds, eval_score\n",
    "\n",
    "    def get_sentence_vector(self, tokens):\n",
    "        vector = np.zeros(self.dim)\n",
    "        for token in tokens:\n",
    "            if token in self.embeddings.keys():\n",
    "                vector += self.embeddings[token]\n",
    "        if not self.average:\n",
    "            vector /= len(tokens)\n",
    "        vector_norm = np.linalg.norm(vector)\n",
    "        if vector_norm != 0:\n",
    "            unit_vector = vector / vector_norm\n",
    "        else:\n",
    "            unit_vector = np.zeros(self.dim)\n",
    "        return unit_vector\n",
    "\n",
    "    def load_or_tokenize_corpus(self, fname):\n",
    "        data = []\n",
    "        if os.path.exists(fname + \"-tokenized\"):\n",
    "            with open(fname + \"-tokenized\", \"r\") as f1:\n",
    "                for line in f1:\n",
    "                    sentence, tokens, label = line.strip().split(\"\\u241E\")\n",
    "                    data.append([sentence, tokens.split(), label])\n",
    "        else:\n",
    "            with open(fname, \"r\") as f2, open(fname + \"-tokenized\", \"w\") as f3:\n",
    "                for line in f2:\n",
    "                    sentence, label = line.strip().split(\"\\u241E\")\n",
    "                    tokens = self.tokenizer.morphs(sentence)\n",
    "                    data.append([sentence, tokens, label])\n",
    "                    f3.writelines(sentence + \"\\u241E\" + ' '.join(tokens) + \"\\u241E\" + label + \"\\n\")\n",
    "        return data\n",
    "\n",
    "    def compute_word_frequency(self, embedding_corpus_fname):\n",
    "        total_count = 0\n",
    "        words_count = defaultdict(int)\n",
    "        with open(embedding_corpus_fname, \"r\") as f:\n",
    "            for line in f:\n",
    "                tokens = line.strip().split()\n",
    "                for token in tokens:\n",
    "                    words_count[token] += 1\n",
    "                    total_count += 1\n",
    "        return words_count, total_count\n",
    "\n",
    "    def load_word_embeddings(self, vecs_fname, method):\n",
    "        if method == \"word2vec\":\n",
    "            model = Word2Vec.load(vecs_fname)\n",
    "            words = model.wv.index2word\n",
    "            vecs = model.wv.vectors\n",
    "        else:\n",
    "            words, vecs = [], []\n",
    "            with open(vecs_fname, 'r', encoding='utf-8') as f1:\n",
    "                if \"fasttext\" in method:\n",
    "                    next(f1)  # skip head line\n",
    "                for line in f1:\n",
    "                    if method == \"swivel\":\n",
    "                        splited_line = line.replace(\"\\n\", \"\").strip().split(\"\\t\")\n",
    "                    else:\n",
    "                        splited_line = line.replace(\"\\n\", \"\").strip().split(\" \")\n",
    "                    words.append(splited_line[0])\n",
    "                    vec = [float(el) for el in splited_line[1:]]\n",
    "                    vecs.append(vec)\n",
    "        return words, vecs\n",
    "\n",
    "    def load_or_construct_weighted_embedding(self, embedding_fname, embedding_method, embedding_corpus_fname, a=0.0001):\n",
    "        dictionary = {}\n",
    "        if os.path.exists(embedding_fname + \"-weighted\"):\n",
    "            # load weighted word embeddings\n",
    "            with open(embedding_fname + \"-weighted\", \"r\") as f2:\n",
    "                for line in f2:\n",
    "                    word, weighted_vector = line.strip().split(\"\\u241E\")\n",
    "                    weighted_vector = [float(el) for el in weighted_vector.split()]\n",
    "                    dictionary[word] = weighted_vector\n",
    "        else:\n",
    "            # load pretrained word embeddings\n",
    "            words, vecs = self.load_word_embeddings(embedding_fname, embedding_method)\n",
    "            # compute word frequency\n",
    "            words_count, total_word_count = self.compute_word_frequency(embedding_corpus_fname)\n",
    "            # construct weighted word embeddings\n",
    "            with open(embedding_fname + \"-weighted\", \"w\") as f3:\n",
    "                for word, vec in zip(words, vecs):\n",
    "                    if word in words_count.keys():\n",
    "                        word_prob = words_count[word] / total_word_count\n",
    "                    else:\n",
    "                        word_prob = 0.0\n",
    "                    weighted_vector = (a / (word_prob + a)) * np.asarray(vec)\n",
    "                    dictionary[word] = weighted_vector\n",
    "                    f3.writelines(word + \"\\u241E\" + \" \".join([str(el) for el in weighted_vector]) + \"\\n\")\n",
    "        return dictionary\n",
    "\n",
    "    def train_model(self, train_data_fname, model_fname):\n",
    "        model = {\"vectors\": [], \"labels\": [], \"sentences\": []}\n",
    "        train_data = self.load_or_tokenize_corpus(train_data_fname)\n",
    "        with open(model_fname, \"w\") as f:\n",
    "            for sentence, tokens, label in train_data:\n",
    "                tokens = self.tokenizer.morphs(sentence)\n",
    "                sentence_vector = self.get_sentence_vector(tokens)\n",
    "                model[\"sentences\"].append(sentence)\n",
    "                model[\"vectors\"].append(sentence_vector)\n",
    "                model[\"labels\"].append(label)\n",
    "                str_vector = \" \".join([str(el) for el in sentence_vector])\n",
    "                f.writelines(sentence + \"\\u241E\" + \" \".join(tokens) + \"\\u241E\" + str_vector + \"\\u241E\" + label + \"\\n\")\n",
    "        return model\n",
    "\n",
    "    def load_model(self, model_fname):\n",
    "        model = {\"vectors\": [], \"labels\": [], \"sentences\": []}\n",
    "        with open(model_fname, \"r\") as f:\n",
    "            for line in f:\n",
    "                sentence, _, vector, label = line.strip().split(\"\\u241E\")\n",
    "                vector = np.array([float(el) for el in vector.split()])\n",
    "                model[\"sentences\"].append(sentence)\n",
    "                model[\"vectors\"].append(vector)\n",
    "                model[\"labels\"].append(label)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "책에서 CBowModel의 핵심이라고 소개한  코드 4-43, 4-44 위주로 살펴보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 코드 4-43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T10:29:44.457537Z",
     "start_time": "2020-03-09T10:29:44.442536Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_word_frequency(self, embedding_corpus_fname):\n",
    "        total_count = 0\n",
    "        words_count = defaultdict(int)\n",
    "        with open(embedding_corpus_fname, 'r') as f:\n",
    "            for line in f:\n",
    "                tokens = line.strip().split()\n",
    "                for token in tokens:\n",
    "                    words_count[token] += 1\n",
    "                    total_count += 1\n",
    "        return words_count, total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문장을 하나씩 읽으면서, 토큰으로 나눈 뒤 \n",
    "등장 순서에 관계없이 사전형태를 만든다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 코드 4-44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T10:29:44.472742Z",
     "start_time": "2020-03-09T10:29:44.458536Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_or_construct_weighted_embedding(self, embedding_fname,\n",
    "                                            embedding_method,\n",
    "                                            embedding_corpus_fname, a=0.0001):\n",
    "        dictionary = {}\n",
    "        \n",
    "        # 가중임베딩을 만든적이 있다면 계산하지 않고 계산한 임베딩을 불러온다.\n",
    "        if os.path.exists(embedding_fname + '-weighted'):\n",
    "            # load weighted word embeddings\n",
    "            with open(embedding_fname + '-weighted', 'r') as f2:\n",
    "                for line in f2:\n",
    "                    word, weighted_vector = line.strip().split('\\u241E')\n",
    "                    weighted_vector = \\\n",
    "                        [float(el) for el in weighted_vector.split()]\n",
    "                    dictionary[word] = weighted_vector\n",
    "                    \n",
    "        # 여기서 부터 하나씩 뜯어보자!\n",
    "        else:\n",
    "            # load pretrained word embeddings\n",
    "            words, vecs = self.load_word_embeddings(embedding_fname, embedding_method)\n",
    "\n",
    "            # compute word frequency\n",
    "            words_count, total_count = compute_word_frequency(embedding_corpus_fname)\n",
    "            \n",
    "            # construct weighted word embeddings\n",
    "            with open(embeding_fname + '-weighted', 'w') as f3:\n",
    "                for word, vec in zip(words, vecs):\n",
    "                    if word in words_count.keys():\n",
    "                        word_prob = words_count[word] / total_count\n",
    "                    else:\n",
    "                        word_prob = 0.0\n",
    "                    weighted_vector = ( a/ (word_prob + a) ) * np.asarray(vec)\n",
    "                    dictionary[word] = weighted_vector\n",
    "                    f3.writelines(word + '\\u241E' + \" \".join([str(el) for el in weighted_vector]) + \"\\n\")\n",
    "        return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T10:29:44.487742Z",
     "start_time": "2020-03-09T10:29:44.473742Z"
    }
   },
   "outputs": [],
   "source": [
    "# words, vecs = self.load_word_embeddings(embedding_fname, embedding_method)\n",
    "\n",
    "def load_word_embeddings(self, vecs_fname, method):\n",
    "    if method == 'word2vec':\n",
    "        model = Word2Vec.load(vecs_fname)\n",
    "        words = model.wv.index2word\n",
    "        vecs = model.wv.vectors\n",
    "    else:\n",
    "        words, vecs = [], []\n",
    "        with open(vecs_fname, 'r', encoding='utf-8') as f1:\n",
    "            if 'fasttext' in method:\n",
    "                next(f1) # skip head line\n",
    "            for line in f1:\n",
    "                if method == 'swivel':\n",
    "                    splited_line = line.replace('\\n', '').strip().split('\\t')\n",
    "                else:\n",
    "                    splited_line = line.replace('\\n', '').strip().split(\" \")\n",
    "                words.append(splited_line[0])\n",
    "                vec = [float(el) for el in splited_line[1:]]\n",
    "                vecs.append(vec)\n",
    "    return words, vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 method에 따른 임베딩 백터를 불러오는 방법이다.\n",
    "\n",
    "word2vec, fasttext, swivel에 방법에 따라서 split할떄, 약간의 차이가 있는것으로 보여진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T10:29:44.503738Z",
     "start_time": "2020-03-09T10:29:44.488738Z"
    }
   },
   "outputs": [],
   "source": [
    "# construct weighted word embeddings\n",
    "# with open(embeding_fname + '-weighted', 'w') as f3:\n",
    "#     for word, vec in zip(words, vecs):\n",
    "#         if word in words_count.keys():\n",
    "#             word_prob = words_count[word] / total_count\n",
    "#         else:\n",
    "#             word_prob = 0.0\n",
    "#         weighted_vector = ( a/ (word_prob + a) ) * np.asarray(vec)\n",
    "#         dictionary[word] = weighted_vector\n",
    "#         f3.writelines(word + '\\u241E' + \" \".join([str(el) for el in weighted_vector]) + \"\\n\")\n",
    "# return dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "words, vecs 는 단어에 따른 임베딩 벡터들이다.\n",
    "\n",
    "words_count.keys()는 **compute_word_frequency**에서 계산한  단어: 단어 빈도수의 사전형태이다\n",
    "\n",
    "word_prob는 words 중 전체 빈도수에서 단어 word가 나온 빈도수를 확률로 나타낸 값\n",
    "\n",
    "weighted_vector에서 단어 등장확률을 반영한 가중치를 곱해준다. (a = 0.0001로 상수이다.)\n",
    "\n",
    "다시 dictionary에서 단어: 가중 임베딩 형태로 return해준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "에러가 나서 아래와 같이 코드에서 약간의 수정을 했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T10:29:44.519738Z",
     "start_time": "2020-03-09T10:29:44.504738Z"
    }
   },
   "outputs": [],
   "source": [
    "#책의 환경이 linux이다 보니, mecab을 쓰기 위해 아래와 같이 수정\n",
    "\n",
    "from konlpy.tag import Okt, Komoran, Hannanum, Kkma\n",
    "from eunjeon import Mecab\n",
    "\n",
    "def get_tokenizer(tokenizer_name):\n",
    "\n",
    "    tokenizer_dict={\n",
    "        'komoran':Komoran(),\n",
    "        'okt':Okt(),\n",
    "        'mecab':Mecab(),\n",
    "        'hannanum':Hannanum(),\n",
    "        'kkma':Kkma()\n",
    "    }\n",
    "    try:\n",
    "        tokenizer=tokenizer_dict[tokenizer_name]\n",
    "    except:\n",
    "        tokenizer=Mecab()\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T10:29:44.535739Z",
     "start_time": "2020-03-09T10:29:44.520740Z"
    }
   },
   "outputs": [],
   "source": [
    "# 이 부분은 이상하게 에러가 나서, word2vec만 남겨두었다.\n",
    "\n",
    "# def load_word_embeddings(vecs_fname, method):\n",
    "#     if method == 'word2vec':\n",
    "#         model = Word2Vec.load(vecs_fname)\n",
    "#         words = model.wv.index2word\n",
    "#         vecs = model.wv.vectors\n",
    "#     return words, vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T11:04:47.258362Z",
     "start_time": "2020-03-09T11:04:47.214442Z"
    }
   },
   "outputs": [],
   "source": [
    "from io import open\n",
    "\n",
    "class CBoWModel(object):\n",
    "\n",
    "    def __init__(self, train_fname, embedding_fname, model_fname, embedding_corpus_fname,\n",
    "                 embedding_method=\"fasttext\", is_weighted=True, average=False, dim=100, tokenizer_name=\"mecab\"):\n",
    "        # configurations\n",
    "        make_save_path(model_fname)\n",
    "        self.dim = dim\n",
    "        self.average = average\n",
    "        if is_weighted:\n",
    "            model_full_fname = model_fname + \"-weighted\"\n",
    "        else:\n",
    "            model_full_fname = model_fname + \"-original\"\n",
    "        self.tokenizer = get_tokenizer(tokenizer_name)\n",
    "        if is_weighted:\n",
    "            # ready for weighted embeddings\n",
    "            self.embeddings = self.load_or_construct_weighted_embedding(embedding_fname, embedding_method, embedding_corpus_fname)\n",
    "            print(\"loading weighted embeddings, complete!\")\n",
    "        else:\n",
    "            # ready for original embeddings\n",
    "            words, vectors = self.load_word_embeddings(embedding_fname, embedding_method)\n",
    "            self.embeddings = defaultdict(list)\n",
    "            for word, vector in zip(words, vectors):\n",
    "                self.embeddings[word] = vector\n",
    "            print(\"loading original embeddings, complete!\")\n",
    "        if not os.path.exists(model_full_fname):\n",
    "            print(\"train Continuous Bag of Words model\")\n",
    "            self.model = self.train_model(train_fname, model_full_fname)\n",
    "        else:\n",
    "            print(\"load Continuous Bag of Words model\")\n",
    "            self.model = self.load_model(model_full_fname)\n",
    "\n",
    "    def evaluate(self, test_data_fname, batch_size=3000, verbose=False):\n",
    "        print(\"evaluation start!\")\n",
    "        test_data = self.load_or_tokenize_corpus(test_data_fname)\n",
    "        data_size = len(test_data)\n",
    "        num_batches = int((data_size - 1) / batch_size) + 1\n",
    "        eval_score = 0\n",
    "        for batch_num in range(num_batches):\n",
    "            batch_sentences = []\n",
    "            batch_tokenized_sentences = []\n",
    "            batch_labels = []\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            features = test_data[start_index:end_index]\n",
    "            for feature in features:\n",
    "                sentence, tokens, label = feature\n",
    "                batch_sentences.append(sentence)\n",
    "                batch_tokenized_sentences.append(tokens)\n",
    "                batch_labels.append(label)\n",
    "            preds, curr_eval_score = self.predict_by_batch(batch_tokenized_sentences, batch_labels)\n",
    "            eval_score += curr_eval_score\n",
    "        if verbose:\n",
    "            for sentence, pred, label in zip(batch_sentences, preds, batch_labels):\n",
    "                print(sentence, \", pred:\", pred, \", label:\", label)\n",
    "        print(\"# of correct:\", str(eval_score), \", total:\", str(len(test_data)), \", score:\", str(eval_score / len(test_data)))\n",
    "\n",
    "    def predict(self, sentence):\n",
    "        tokens = self.tokenizer.morphs(sentence)\n",
    "        sentence_vector = self.get_sentence_vector(tokens)\n",
    "        scores = np.dot(self.model[\"vectors\"], sentence_vector)\n",
    "        pred = self.model[\"labels\"][np.argmax(scores)]\n",
    "        return pred\n",
    "\n",
    "    def predict_by_batch(self, tokenized_sentences, labels):\n",
    "        sentence_vectors, eval_score = [], 0\n",
    "        for tokens in tokenized_sentences:\n",
    "            sentence_vectors.append(self.get_sentence_vector(tokens))\n",
    "        scores = np.dot(self.model[\"vectors\"], np.array(sentence_vectors).T)\n",
    "        preds = np.argmax(scores, axis=0)\n",
    "        for pred, label in zip(preds, labels):\n",
    "            if self.model[\"labels\"][pred] == label:\n",
    "                eval_score += 1\n",
    "        return preds, eval_score\n",
    "\n",
    "    def get_sentence_vector(self, tokens):\n",
    "        vector = np.zeros(self.dim)\n",
    "        for token in tokens:\n",
    "            if token in self.embeddings.keys():\n",
    "                vector += self.embeddings[token]\n",
    "        if not self.average:\n",
    "            vector /= len(tokens)\n",
    "        vector_norm = np.linalg.norm(vector)\n",
    "        if vector_norm != 0:\n",
    "            unit_vector = vector / vector_norm\n",
    "        else:\n",
    "            unit_vector = np.zeros(self.dim)\n",
    "        return unit_vector\n",
    "\n",
    "    def load_or_tokenize_corpus(self, fname):\n",
    "        data = []\n",
    "        if os.path.exists(fname + \"-tokenized\"):\n",
    "            with open(fname + \"-tokenized\", \"r\") as f1:\n",
    "                for line in f1:\n",
    "                    sentence, tokens, label = line.strip().split(\"\\u241E\")\n",
    "                    data.append([sentence, tokens.split(), label])\n",
    "        else:\n",
    "            with open(fname, \"r\") as f2, open(fname + \"-tokenized\", \"w\") as f3:\n",
    "                for line in f2:\n",
    "                    sentence, label = line.strip().split(\"\\u241E\")\n",
    "                    tokens = self.tokenizer.morphs(sentence)\n",
    "                    data.append([sentence, tokens, label])\n",
    "                    f3.writelines(sentence + \"\\u241E\" + ' '.join(tokens) + \"\\u241E\" + label + \"\\n\")\n",
    "        return data\n",
    "\n",
    "\n",
    "    def compute_word_frequency(self, embedding_corpus_fname):\n",
    "        total_count = 0\n",
    "        words_count = defaultdict(int)\n",
    "        with open(embedding_corpus_fname, \"r\") as f:\n",
    "            for line in f:\n",
    "                tokens = line.strip().split()\n",
    "                for token in tokens:\n",
    "                    words_count[token] += 1\n",
    "                    total_count += 1\n",
    "        return words_count, total_count\n",
    "\n",
    "    def load_word_embeddings(self, vecs_fname, method):\n",
    "        if method == \"word2vec\":\n",
    "            model = Word2Vec.load(vecs_fname)\n",
    "            words = model.wv.index2word\n",
    "            vecs = model.wv.vectors\n",
    "        return words, vecs\n",
    "\n",
    "    def load_or_construct_weighted_embedding(self, embedding_fname, embedding_method, embedding_corpus_fname, a=0.0001):\n",
    "        dictionary = {}\n",
    "        if os.path.exists(embedding_fname + \"-weighted\"):\n",
    "            # load weighted word embeddings\n",
    "            with open(embedding_fname + \"-weighted\", \"r\") as f2:\n",
    "                for line in f2:\n",
    "                    word, weighted_vector = line.strip().split(\"\\u241E\")\n",
    "                    weighted_vector = [float(el) for el in weighted_vector.split()]\n",
    "                    dictionary[word] = weighted_vector\n",
    "        else:\n",
    "            # load pretrained word embeddings\n",
    "            words, vecs = self.load_word_embeddings(embedding_fname, embedding_method)\n",
    "            # compute word frequency\n",
    "            words_count, total_word_count = self.compute_word_frequency(embedding_corpus_fname)\n",
    "            # construct weighted word embeddings\n",
    "            with open(embedding_fname + \"-weighted\", \"w\") as f3:\n",
    "                for word, vec in zip(words, vecs):\n",
    "                    if word in words_count.keys():\n",
    "                        word_prob = words_count[word] / total_word_count\n",
    "                    else:\n",
    "                        word_prob = 0.0\n",
    "                    weighted_vector = (a / (word_prob + a)) * np.asarray(vec)\n",
    "                    dictionary[word] = weighted_vector\n",
    "                    f3.writelines(word + \"\\u241E\" + \" \".join([str(el) for el in weighted_vector]) + \"\\n\")\n",
    "        return dictionary\n",
    "\n",
    "    def train_model(self, train_data_fname, model_fname):\n",
    "        model = {\"vectors\": [], \"labels\": [], \"sentences\": []}\n",
    "        train_data = self.load_or_tokenize_corpus(train_data_fname)\n",
    "        with open(model_fname, \"w\") as f:\n",
    "            for sentence, tokens, label in train_data:\n",
    "                tokens = self.tokenizer.morphs(sentence)\n",
    "                sentence_vector = self.get_sentence_vector(tokens)\n",
    "                model[\"sentences\"].append(sentence)\n",
    "                model[\"vectors\"].append(sentence_vector)\n",
    "                model[\"labels\"].append(label)\n",
    "                str_vector = \" \".join([str(el) for el in sentence_vector])\n",
    "                f.writelines(sentence + \"\\u241E\" + \" \".join(tokens) + \"\\u241E\" + str_vector + \"\\u241E\" + label + \"\\n\")\n",
    "        return model\n",
    "\n",
    "    def load_model(self, model_fname):\n",
    "        model = {\"vectors\": [], \"labels\": [], \"sentences\": []}\n",
    "        with open(model_fname, \"r\") as f:\n",
    "            for line in f:\n",
    "                sentence, _, vector, label = line.strip().split(\"\\u241E\")\n",
    "                vector = np.array([float(el) for el in vector.split()])\n",
    "                model[\"sentences\"].append(sentence)\n",
    "                model[\"vectors\"].append(vector)\n",
    "                model[\"labels\"].append(label)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T11:03:37.106304Z",
     "start_time": "2020-03-09T11:03:37.094302Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_save_path(full_path):\n",
    "    if full_path[:4] == \"data\":\n",
    "        full_path = os.path.join(os.path.abspath(\".\"), full_path)\n",
    "    model_path = '/'.join(full_path.split(\"/\")[:-1])\n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T11:03:37.375233Z",
     "start_time": "2020-03-09T11:03:37.373232Z"
    }
   },
   "outputs": [],
   "source": [
    "train_fname = '../data/processed/processed_ratings_train.txt'\n",
    "embedding_fname = '../data/word-embeddings/word2vec/word2vec'\n",
    "model_fname = '../data/word-embeddings/cbow/word2vec'\n",
    "embedding_corpus_fname = '../data/tokenized/corpus_mecab.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T11:04:51.919429Z",
     "start_time": "2020-03-09T11:04:48.992362Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading original embeddings, complete!\n",
      "train Continuous Bag of Words model\n"
     ]
    }
   ],
   "source": [
    "model = CBoWModel(train_fname, embedding_fname, model_fname, embedding_corpus_fname,\n",
    "                  embedding_method='word2vec', is_weighted=False, average=False, dim=100, tokenizer_name='mecab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습은 되는데 평가가 되지 않는다.\n",
    "\n",
    "아무래도 encoding 문제로 생각된다.\n",
    "\n",
    "@load_or_tokenize_corpus에서 split할때 자꾸 에러가 뜬다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T11:04:53.103210Z",
     "start_time": "2020-03-09T11:04:53.072305Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation start!\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "zip argument #1 must support iteration",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-b3c039498603>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtest_corpus_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'../data/processed/processed_ratings_test.txt'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_corpus_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-73-0bc909d62226>\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, test_data_fname, batch_size, verbose)\u001b[0m\n\u001b[0;32m     50\u001b[0m                 \u001b[0mbatch_tokenized_sentences\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m                 \u001b[0mbatch_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m             \u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurr_eval_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_by_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_tokenized_sentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m             \u001b[0meval_score\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mcurr_eval_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-73-0bc909d62226>\u001b[0m in \u001b[0;36mpredict_by_batch\u001b[1;34m(self, tokenized_sentences, labels)\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"vectors\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence_vectors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"labels\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m                 \u001b[0meval_score\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: zip argument #1 must support iteration"
     ]
    }
   ],
   "source": [
    "test_corpus_path = '../data/processed/processed_ratings_test.txt'\n",
    "\n",
    "model.evaluate(test_corpus_path, 3000, False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('vscode': conda)",
   "language": "python",
   "name": "python37464bitvscodeconda5de396a6d5a74089a4a1b59718a6a1ff"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
