{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Is All You Need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://github.com/bentrevett/pytorch-seq2seq/raw/e8209a7b0207cde55871be352819cac3dd5c05ce/assets/transformer1.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T09:13:05.670996Z",
     "start_time": "2020-04-07T09:13:02.083250Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchtext\n",
    "from torchtext.datasets import TranslationDataset, Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T09:13:35.525203Z",
     "start_time": "2020-04-07T09:13:35.506680Z"
    }
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T09:13:35.801831Z",
     "start_time": "2020-04-07T09:13:35.714026Z"
    }
   },
   "outputs": [],
   "source": [
    "seed_everything(1234)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T09:13:37.527657Z",
     "start_time": "2020-04-07T09:13:36.195261Z"
    }
   },
   "outputs": [],
   "source": [
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T09:13:37.543657Z",
     "start_time": "2020-04-07T09:13:37.528659Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_de(text):\n",
    "    \"\"\"\n",
    "    Tokenizes German text from a string into a list of strings\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T09:13:38.423241Z",
     "start_time": "2020-04-07T09:13:38.406305Z"
    }
   },
   "outputs": [],
   "source": [
    "SRC = Field(tokenize = tokenize_de, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)\n",
    "\n",
    "TRG = Field(tokenize = tokenize_en, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T09:13:41.825893Z",
     "start_time": "2020-04-07T09:13:38.815141Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'), fields = (SRC, TRG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T09:13:43.414123Z",
     "start_time": "2020-04-07T09:13:43.235791Z"
    }
   },
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_data, min_freq = 2)\n",
    "TRG.build_vocab(train_data, min_freq = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T09:13:43.508734Z",
     "start_time": "2020-04-07T09:13:43.495724Z"
    }
   },
   "outputs": [],
   "source": [
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENCODER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://github.com/bentrevett/pytorch-seq2seq/raw/e8209a7b0207cde55871be352819cac3dd5c05ce/assets/transformer-encoder.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이전의 기존 Attention모델에서처럼, Source문장을 하나의 Context Vector $z$로 나타내지 않습니다.\n",
    "\n",
    "대신에, Soruce문장 길이에 맞는 Sequence of Context Vectors $Z = (z_1, ... , z_n)$를 만듭니다.\n",
    "\n",
    "* *example: len(source):5이면, $Z = (z_1, z_2, z_3, z_4, z_5)$*\n",
    "\n",
    "왜 sequence of hidden states라 부르지 않고, sequence of context vectors라 부르는 이유는 무엇일까?\n",
    "\n",
    "$t$시점의 hidden state는 $x_t$의 token과 그 전까지의 $h_{t-1}$을 보지만,\n",
    "\n",
    "context vector는 source문장 내의 모든 위치의 token을 취합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫번쨰로, token들이 embedding layer를 통과합니다.\n",
    "\n",
    "다음에는, 모델이 input sequence안에서의 token들의 순서를 모르기 때문에, \n",
    "\n",
    "positional embedding layer라 불리는 두 번쨰 embedding layer를 통과시킵니다.\n",
    "\n",
    "첫번쨰 embedding layer에서의 input은 token 그 자체가 아니라, sequence안의 token 위치입니다.\n",
    "\n",
    "첫 token, $<SOS>$ token, posion 0부터 시작합니다.\n",
    "    \n",
    "position embedding은 vocab의 사이즈를 100으로 하였을떄, 모델은 문장을 100개의 token으로 인식합니다. \n",
    "\n",
    "원래의 Attention is All You Need 논문에서 나온 Transfomer는 positional embedding을 학습하지 않습니다.\n",
    "\n",
    "대신에 고정된 길이의 embedding을 사용하였습니다.\n",
    "\n",
    "하지만 BERT처럼  최신의 Transformer구조는 positional embedding을 사용합니다.\n",
    "\n",
    "따라서 이 chapter에서도 positional embedding을 사용합니다.\n",
    "\n",
    "다음에는 token과 postional embedding을 elementwise 방식으로 더하여 하나의 vector를 얻습니다.\n",
    "\n",
    "이 vector에는 위치와 token에 대한 정보를 포함합니다.\n",
    "\n",
    "합치기 전에는 scaling factor which is $\\sqrt{d_{model}}$, where $d_{model}$ is the hidden dimension size, hid_dim를 token embedding과 곱해주어야 합니다.\n",
    "\n",
    "이것은 임베딩에서의 분산을 감소시키는 것으로 추정되며, scaling factor가 없으면 모델을 안정적으로 훈련시키기가 어렵습니다.\n",
    "\n",
    "결합된 embeddings들은 Dropout을 적용합니다.\n",
    "\n",
    "결합된 embeddings는 $N$개의 encoder layer를 통과 후, $Z$를 얻게되며, 다시 decoder에 의해 사용됩니다.\n",
    "\n",
    "source mask, src_mask는 간단히 source 문장에서의 같은 shape를 가지면서, $<pad>$ token일 경우 0, 아닐 경우 1을 가지게 됩니다.\n",
    "\n",
    "이것이 encoder layer에 사용된 multi-head attention mechanism의 원리이며,soruce 문장 내에서 attention을 적용합니다.\n",
    "\n",
    "모델은 $<pad>$tokden에 attention하지 않고, 쓸모없는 정보를 포함하지 않습니다.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T09:13:08.943531Z",
     "start_time": "2020-04-07T09:13:08.923533Z"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hid_dim, n_layers, n_heads, pf_dim, dropout, device, max_length = 100):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        \n",
    "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        \n",
    "        self.layers = nn.ModuleList([EncoderLayer(hid_dim, n_heads, pf_dim, dropout, device) for _ in range(n_layers)])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #src_mask = [batch size, src len]\n",
    "        \n",
    "        batch_size = src.shape[0]\n",
    "        src_len = src.shape[1]\n",
    "        \n",
    "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        \n",
    "        #pos = [batch size, src len]\n",
    "        \n",
    "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
    "        \n",
    "        #src = [batch size, src len, hid dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "            \n",
    "        #src = [batch size, src len, hid dim]\n",
    "            \n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T09:20:40.860326Z",
     "start_time": "2020-04-07T09:20:40.785306Z"
    }
   },
   "outputs": [],
   "source": [
    "batch = next(iter(train_iterator))\n",
    "test_src = batch.src\n",
    "test_src = test_src.view(128,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T09:22:41.350291Z",
     "start_time": "2020-04-07T09:22:41.307196Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 2, 2,  ..., 2, 2, 2],\n",
       "        [2, 2, 2,  ..., 2, 2, 2],\n",
       "        [2, 2, 2,  ..., 2, 2, 2],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T09:20:49.111558Z",
     "start_time": "2020-04-07T09:20:49.104044Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 36])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_src.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T09:21:32.913716Z",
     "start_time": "2020-04-07T09:21:32.908715Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = test_src.shape[0]\n",
    "src_len = test_src.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T09:22:31.193709Z",
     "start_time": "2020-04-07T09:22:31.187165Z"
    }
   },
   "outputs": [],
   "source": [
    "pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T09:22:31.867242Z",
     "start_time": "2020-04-07T09:22:31.843898Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  ..., 33, 34, 35],\n",
       "        [ 0,  1,  2,  ..., 33, 34, 35],\n",
       "        [ 0,  1,  2,  ..., 33, 34, 35],\n",
       "        ...,\n",
       "        [ 0,  1,  2,  ..., 33, 34, 35],\n",
       "        [ 0,  1,  2,  ..., 33, 34, 35],\n",
       "        [ 0,  1,  2,  ..., 33, 34, 35]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://yjucho1.github.io/assets/img/2018-10-13/transformer-encoder.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder layers are where all of the \"meat\" of the encoder is contained. \n",
    "\n",
    "encoder layers들은 모두 multi head attention layer가 포함되어 있습니다.\n",
    "\n",
    ">We first pass the source sentence and its mask into the multi-head attention layer, then perform dropout on it, apply a residual >connection and pass it through a **Layer Normalization layer**.\n",
    ">\n",
    ">We then pass it through a position-wise feedforward layer and then, again, apply dropout, a residual connection and then layer >normalization to get the output of this layer which is fed into the next layer.\n",
    ">\n",
    ">The parameters are not shared between layers.\n",
    ">\n",
    "mutli head attention layer는 encoder layer에 의해, source 문장을 attend 하는데 사용됩니다.\n",
    "\n",
    "즉, 다른 sequence 대신 자체적으로 atteintion를 계산하고 적용하므로 self attention이라고 합니다.\n",
    "\n",
    "이를 통해 Transformer와 같이 더 많은 계층의 신경망을 쉽게 학습 할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T10:42:47.983747Z",
     "start_time": "2020-04-07T10:42:47.966751Z"
    }
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim,  dropout, device):\n",
    "        super().__init__()\n",
    "        self.layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        \n",
    "        #src = [batch size, src len, hid dim]\n",
    "        #src_mask = [batch size, src len]\n",
    "                \n",
    "        #self attention\n",
    "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
    "        \n",
    "        #dropout, residual connection and layer norm\n",
    "        src = self.layer_norm(src + self.dropout(_src))\n",
    "        \n",
    "        #src = [batch size, src len, hid dim]\n",
    "        \n",
    "        #positionwise feedforward\n",
    "        _src = self.positionwise_feedforward(src)\n",
    "        \n",
    "        #dropout, residual and layer norm\n",
    "        src = self.layer_norm(src + self.dropout(_src))\n",
    "        \n",
    "        #src = [batch size, src len, hid dim]\n",
    "        \n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutli Head Attention Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://github.com/bentrevett/pytorch-seq2seq/raw/e8209a7b0207cde55871be352819cac3dd5c05ce/assets/transformer-attention.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-Head Attention의 역할은 1문장을 여러 head로 Self-Attention 시킴에 있습니다.\n",
    "\n",
    "$$ \\text{Attention}(Q, K, V) = \\text{Softmax} \\big( \\frac{QK^T}{\\sqrt{d_k}} \\big)V $$\n",
    "\n",
    "* Query(Q) : 영향을 받는 단어 A를 나타내는 변수입니다.\n",
    "* Key(K) : 영향을 주는 단어 B를 나타내는 변수입니다.\n",
    "* Value(V) : 그 영향에 대한 가중치를 나타냅니다.\n",
    "\n",
    "\n",
    "\"Je suis étudiant\" 라는 문장의 임베딩 벡터가 512차원이라면 8개 head로 나눠 64개의 벡터를 한 Scaled Dot Attention이 맡아 처리하는 것입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T11:03:32.512399Z",
     "start_time": "2020-04-07T11:03:32.475357Z"
    }
   },
   "outputs": [],
   "source": [
    "# self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "# _src, _ = self.self_attention(src, src, src, src_mask)\n",
    "\n",
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert hid_dim % n_heads == 0\n",
    "        \n",
    "        self.hid_dim = hid_dim \n",
    "        self.n_heads = n_heads \n",
    "        self.head_dim = hid_dim // n_heads \n",
    "        \n",
    "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "        \n",
    "    def forward(self, query, key, value, mask = None):\n",
    "        \n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        #query = [batch size, query len, hid dim]\n",
    "        #key = [batch size, key len, hid dim]\n",
    "        #value = [batch size, value len, hid dim]\n",
    "                \n",
    "        Q = self.fc_q(query)\n",
    "        K = self.fc_k(key)\n",
    "        V = self.fc_v(value)\n",
    "        \n",
    "        #Q = [batch size, query len, hid dim]\n",
    "        #K = [batch size, key len, hid dim]\n",
    "        #V = [batch size, value len, hid dim]\n",
    "                \n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        \n",
    "        #Q = [batch size, n heads, query len, head dim]\n",
    "        #K = [batch size, n heads, key len, head dim]\n",
    "        #V = [batch size, n heads, value len, head dim]\n",
    "                \n",
    "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
    "        \n",
    "        #energy = [batch size, n heads, query len, key len]\n",
    "        \n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        attention = torch.softmax(energy, dim = -1)\n",
    "                \n",
    "        #attention = [batch size, n heads, query len, key len]\n",
    "                \n",
    "        x = torch.matmul(self.dropout(attention), V)\n",
    "        \n",
    "        #x = [batch size, n heads, query len, head dim]\n",
    "        \n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        \n",
    "        #x = [batch size, query len, n heads, head dim]\n",
    "        \n",
    "        x = x.view(batch_size, -1, self.hid_dim)\n",
    "        \n",
    "        #x = [batch size, query len, hid dim]\n",
    "        \n",
    "        x = self.fc_o(x)\n",
    "        \n",
    "        #x = [batch size, query len, hid dim]\n",
    "        \n",
    "        return x, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position-wise Feedforward Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 head가 만들어낸 Self-Attention을 치우치지 않게 균등하게 섞는 역할을 합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T12:27:34.250214Z",
     "start_time": "2020-04-07T12:27:34.236188Z"
    }
   },
   "outputs": [],
   "source": [
    "class PositionwiseFeedforwardLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n",
    "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #x = [batch size, seq len, hid dim]\n",
    "        \n",
    "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
    "        \n",
    "        #x = [batch size, seq len, pf dim]\n",
    "        \n",
    "        x = self.fc_2(x)\n",
    "        \n",
    "        #x = [batch size, seq len, hid dim]\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('vscode': conda)",
   "language": "python",
   "name": "python37464bitvscodeconda5de396a6d5a74089a4a1b59718a6a1ff"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
